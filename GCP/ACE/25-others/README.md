- [Google Cloud Deployment Manager](#google-cloud-deployment-manager)
  - [Introduction](#introduction)
  - [Advantages](#advantages)
  - [Understanding](#understanding)
  - [Cloud DEployment manager script example](#cloud-deployment-manager-script-example)
  - [Terminology](#terminology)
- [Cloud Marketplace(Cloud Launcher)](#cloud-marketplacecloud-launcher)
- [Cloud DNS](#cloud-dns)
  - [Cloud DNS CLI](#cloud-dns-cli)
- [Google Dataflow](#google-dataflow)
- [Google Cloud Dataproc](#google-cloud-dataproc)


# Google Cloud Deployment Manager
## Introduction
- Let's consider an example:
  - I would want to create a new VPC and a subnet
  - I want to provision a Load Balancer, Instance groups with 5 compute Engine instancees and Cloud SQL database in the subnet
  - I would want to setup the right firewall
- AND I would want to create 4 environments
  - Dev, QA, Stage and Production
- Deployment manager can help us do all these with a simple(Actually NOT so simple) script.
## Advantages
- Automate deployment and modification of Google Cloud resources in a controlled, predictable way.
  - Deploy in multiple environments easily.
- Avoid configruration drift
- Avoid mistakes with manual configuration.
- Think of it as a version control for your environments.
- IMP: Always modify the resources created by Deployment manager using Deployment manager.

## Understanding
- All configuration is defined in a simple text file - YAML
  - I want a VPC, a subnet, a database and..
- Deployment manager understands dependencies
  - Create VPCs first, then subnets and then database
- (DEFAULT) Automatic rollbacks on errors(Easier to retry)
  - If creation of database fails, it would automatic delete the subnet and VPC.
- Version control your configuration file and make changes to it over time.
- Free to use - Pay only for the resources provisioned.
  - Get an automated estimate for your configuration.

## Cloud DEployment manager script example
```yaml
- type: compute.v1.instance
  name: my-first-vm
  properties:
    zone: us-central1-a
    machineType: <<MACHINE_TYPE>
    disks:
      - deviceName: boot
        type: PERSISTENT
        autoDelete: true
        initializeParams:
          sourceImage: <<SOURCE_IMAGE>
    networkInterfaces:
      - network: <<NETWORK>
        accessConfigs:
          - name: External NAT
            type: ONE_TO_ONE_NAT
```

## Terminology
- Configuration file: Yaml file with resources definitions for a single deployment
- Templates: Reusable resource definitions that can be used in multiple configuration files.
  - Can be defined using
    - Python(preferred)
    - JinJa2(recommended only for very simple scripts)
- Deployment: Collection of resources that are deployed and managed together
- Manifests: Read-only object containing original deployment configuration(including imported templates)
  - Generated by Deployment manager
  - Includes fully-expanded resource list
  - Helpful for troubleshooting


# Cloud Marketplace(Cloud Launcher)
- Installing custom software might involve setting up multiple resources:
  - Example: Installing wordpress needs set up of compute engine and a relational database
- How do you simplify the setup of custom software solutions like wordpress or even more complex things like SAP HANA suite on GCP?
- Cloud Marketplace: Central repo of easily deployable apps & datasets
  - Similar to App Store/Play store for mobile applications
  - You can search and install a complete stack
    - Commercial Solutions: SAP HANA etc.
    - Open Source Packages: LAMP, Wordpress, Cassandra, Jenkins etc.
    - OS Licenses: BYOL, Free, Paid
    - Categories: Datasets/Developer tools/OS etc.
- When selecting a solution, you can see:
  - Components - software, infrastructure needed etc.
  - Approximate price.

# Cloud DNS
- What would be the steps in setting up a website with a domain name
  - Step 1: Buy the domain name in Domain registrar.
  - Step 2: Setup your website content(Website hosting)
  - Step 3: Route requests to domain to the my website host server(DNS)
- Cloud DNS = Global Domain Name System
  - Setup your DNS routing for your website
    - Route api.tushar.com to the IP Address of api server
    - Route static.tushar.com to the IP Address of https server
  - Public and Private managed DNS zones(Container for records)
- Create DNS Zone(Public or private)
- Choose network for private zone
- Add records
## Cloud DNS CLI
- ```gcloud dns managed-zones create ZONE_NAME```
  - ```--description```(Required - short description for managed zone)
  - ```--dns-name```(Required - DNS name suffix that will be managed with the created zone)
  - ```--visibility```(private/public)
  - ```--networks```(List of networks that the zone should be visible in if the zone visibility is [private])
- Start transaction for zone
  - ```gcloud dns record-sets transaction start --zone```
- Make changes
  - ```gcloud dns record-sets transaction add --name=REC_NAME --ttl --type A/CNAME --zone=ZONE_NAME```
- End transaction for zone.

# Google Dataflow
- Difficult to describe.
- Provides unified streaming and batch data processing that's serverless.
- Built upon an Open-Source technology: Apache Beam.
- Job Templates
- Example pipelines:
  - Pub/Sub > Dataflow > BigQuery(Streaming)
  - Pub/Sub > Dataflow > Cloud Storage(Streaming - files)
  - Cloud Storage > Dataflow > Bigtable/CloudSpanner/Datastore/BigQuery(Batch - load data into databases)
  - Bulk compress files in Cloud storage(Batch)
  - Convert file formats between Avro, Parquet & csv(Batch)
- Straming and batch Usecases
  - Realtime Fraud Detection, Sensor Data processing, Log Data processing, Batch processing(Load Data, convert formats etc.)
- Use pre-built templates
- Based on Apache beam(Supports java, python, Go)
- Serverless(and Autoscaling)

# Google Cloud Dataproc
- Managed Spark and Hadoop service.
  - Variety of jobs are supported:
    - Spark, PySpark, SparkR, Hive, SparkSQL, Pig, Hadoop
  - Perform complex batch processing
- Mulitiple cluster modes:
  - Single node/standard/high availability(3 masters)
  - Use regular/preemptible VMs
- Use Case: Move your hadoop and Spark clusters to the cloud  
  - Perform your machine learning and AI Development using open source frameworks.
- REMEMBER: Cloud Dataproc is a data analysis platform
  - You can export cluster configuration but NOT Data.
- ALTERNATIVE: BIGQUERY: When you run SQL queries on Petabytes.
  - Go for Cloud Dataproc when you need more than queries(Example: Complex batch processing, Machine Learning and AI workloads.)
